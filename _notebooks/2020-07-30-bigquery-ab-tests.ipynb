{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"A/B tests with hashes in BigQuery\"\n",
    "\n",
    "- author: bbernst\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [bigquery, \"ab tests\"]\n",
    "- image: images/bigquery-ab-tests/dists.png\n",
    "- hide: false\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An example problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I've needed to generate random but consistent values in a BigQuery data pipeline. This is a pipeline that runs many times per day and constantly has new data feeding in through upstream pipes.\n",
    "\n",
    "As an example, let's say that we have 10,000 users and need to randomly place them into groups depending on the hour of the work day. Additionally, we only want to do this once; once a user gets placed into a group, they stay in that group. This way, downstream work can rely on how each user is assigned without having to worry about checking for changes. Visually, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>user_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>~1000 random users</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   hour            user_ids\n",
       "0     9  ~1000 random users\n",
       "1    10  ~1000 random users\n",
       "2    11  ~1000 random users\n",
       "3    12  ~1000 random users\n",
       "4    13  ~1000 random users\n",
       "5    14  ~1000 random users\n",
       "6    15  ~1000 random users\n",
       "7    16  ~1000 random users\n",
       "8    17  ~1000 random users\n",
       "9    18  ~1000 random users"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "hours = range(9, 19)\n",
    "pd.DataFrame({'hour': hours, 'user_ids': ['~1000 random users']*len(hours)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigQuery has a number of functions for producing random numbers, like `RAND()`, and we can create the table above with a query:\n",
    "\n",
    "```sql\n",
    "WITH src AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    CAST(FLOOR(9 + 10 * RAND()) AS INT64) AS hour\n",
    "  FROM UNNEST(GENERATE_ARRAY(1, 10000)) user_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  hour,\n",
    "  ARRAY_AGG(user_id) AS user_ids\n",
    "FROM src\n",
    "GROUP BY hour\n",
    "ORDER BY hour\n",
    "```\n",
    "\n",
    "However, this query produces *NEW* random groups each time the query is run. There's no built-in way to specify a seed in `RAND()`, so we'll need to learn how to create a `CONSISTENT_RAND(...)` ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [thorough post](http://blog.richardweiss.org/2016/12/25/hash-splits.html) solves our problem, gives us python and SQL code, but unfortunately not for the BigQuery dialect.\n",
    "\n",
    "Let's build this up for BigQuery step by step within the `src` CTE:\n",
    "\n",
    "```sql\n",
    "  SELECT\n",
    "    user_id,\n",
    "    CONCAT(user_id, 'salt') AS key,\n",
    "    MD5(CONCAT(user_id, 'salt')) AS md5_key,\n",
    "    TO_HEX(MD5(CONCAT(user_id, 'salt'))) AS hex_md5_key,\n",
    "    SUBSTR(TO_HEX(MD5(CONCAT(user_id, 'salt'))), 1, 6) AS substr_hex_md5_key,\n",
    "    CONCAT('0x', SUBSTR(TO_HEX(MD5(CONCAT(user_id, 'salt'))), 1, 6)) correct_hex,\n",
    "    CAST(CONCAT('0x', SUBSTR(TO_HEX(MD5(CONCAT(user_id, 'salt'))), 1, 6)) AS INT64) AS numerator,\n",
    "    CAST('0xFFFFFF' AS INT64) AS denominator,\n",
    "    CAST(CONCAT('0x', SUBSTR(TO_HEX(MD5(CONCAT(user_id, 'salt'))), 1, 6)) AS INT64) / \n",
    "      CAST('0xFFFFFF' AS INT64) AS consistent_rand\n",
    "  FROM UNNEST(GENERATE_ARRAY(1, 10000)) user_id\n",
    "```\n",
    "\n",
    "1. `key`: Allows us to effectively set a seed using `user_id` and a `salt`\n",
    "1. `md5_key`: Hash the `key` with `MD5()`\n",
    "1. `hex_md5_key`: Convert `md5_key` with `TO_HEX()`\n",
    "1. `substr_hex_md5_key`: Take the first 6 digits of `hex_md5_key`\n",
    "1. `correct_hex`: Prepend 0x to `correct_hex` since all hex values start with 0x\n",
    "1. `numerator`: Convert `correct_hex` to an integer because we want to get to a number\n",
    "1. `denominator`: Convert 0xFFFFFF to an integer because it's the largest 6 digit hex value\n",
    "1. `consistent_rand`: Divide `numerator` and `denominator` to get a value between 0 and 1\n",
    "\n",
    "And putting this all together for our problem:\n",
    "\n",
    "```sql\n",
    "WITH src AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    CAST(FLOOR(9 + 10 *\n",
    "      CAST(CONCAT('0x', SUBSTR(TO_HEX(MD5(CONCAT(user_id, 'salt'))), 1, 6)) AS INT64) / \n",
    "      CAST('0xFFFFFF' AS INT64)\n",
    "    ) AS INT64) AS hour\n",
    "  FROM UNNEST(GENERATE_ARRAY(1, 10000)) user_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  hour, \n",
    "  ARRAY_AGG(user_id) AS user_ids\n",
    "FROM src\n",
    "GROUP BY hour\n",
    "ORDER BY hour\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved the goal, but we can clean this up to save ourselves some copy and paste steps in the future. BigQuery let's us define UDFs (user defined functions):\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE FUNCTION `project.dataset.CONSISTENT_RAND` (\n",
    "  key STRING\n",
    ")\n",
    "RETURNS FLOAT64 AS (\n",
    "  CAST(CONCAT('0x', SUBSTR(TO_HEX(MD5(key)), 1, 6)) AS INT64) / \n",
    "  CAST('0xFFFFFF' AS INT64)\n",
    ")\n",
    "```\n",
    "\n",
    "Now, it's a more convenient query to write:\n",
    "\n",
    "```sql\n",
    "WITH src AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    CAST(FLOOR(9 + 10 *\n",
    "      `project.dataset.CONSISTENT_RAND`(CONCAT(user_id, 'salt'))\n",
    "    ) AS INT64) AS hour\n",
    "  FROM UNNEST(GENERATE_ARRAY(1, 10000)) user_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  hour, \n",
    "  ARRAY_AGG(user_id) AS user_ids\n",
    "FROM src\n",
    "GROUP BY hour\n",
    "ORDER BY hour\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside on Inverse Transform Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something implicit in the above example is the mapping from a uniform random number in [0,1], the result of `RAND()` and `CONSISTENT_RAND()`, to a uniform random number between [9, 19], the hour. It's pretty intuitive that you can take something uniform, space the points out a bit, and it stays uniform, but there's also theory behind it -- [Inverse transform sampling](https://en.wikipedia.org/wiki/Inverse_transform_sampling).\n",
    "\n",
    "For the uniform to uniform case we have our sampler, $ U \\sim Unif(0,1) $ and our hours, $ H \\sim Unif(9,19) $. \n",
    "\n",
    "The [CDF](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)#Cumulative_distribution_function) for H is:\n",
    "$$\n",
    "F(x) = \\frac{x-9}{10}\n",
    "$$\n",
    "And we want to solve:\n",
    "$$\n",
    "F(F^{-1}(u)) = u \\\\\n",
    "\\frac{F^{-1}(u)-9}{10} = u \\\\\n",
    "F^{-1}(u) = 9 + 10 * u\n",
    "$$\n",
    "\n",
    "Check it out! We found $ 9 + 10 * u $ which is exactly what we've be writing in our queries: `9 + 10 * CONSISTENT_RAND(key)`.\n",
    "\n",
    "Now, what if we switch the problem to something slightly different and instead of spreading out our users evenly throughout the day, we want them to spread out according to an exponential distribution. This way more users fall into earlier hours and fewer into later hours -- maybe the downstream work is easier to do early in the day before people get tired. Also let's forget about the working day hours for this part.\n",
    "\n",
    "For the uniform to exponential case we have our sampler, $ U \\sim Unif(0,1) $ and our hours, $ H \\sim Expon(1) $.\n",
    "\n",
    "The [CDF](https://en.wikipedia.org/wiki/Exponential_distribution#Cumulative_distribution_function) for H is:\n",
    "$$\n",
    "F(x) = 1 - e^{-x}\n",
    "$$\n",
    "And, like before, we want to solve:\n",
    "$$\n",
    "F(F^{-1}(u)) = u \\\\\n",
    "1 - e^{-F^{-1}(u)} = u \\\\\n",
    "e^{-F^{-1}(u)} = 1 - u \\\\\n",
    "-F^{-1}(u) = ln(1 - u) \\\\\n",
    "F^{-1}(u) = -ln(1 - u)\n",
    "$$\n",
    "\n",
    "In BigQuery we can write `-1 * LN(1 - CONSISTENT_RAND(key))` and our groups will follow the exponential distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be asking, what does this have to do with A/B tests? When creating an A/B test, we often want to:\n",
    "1. Randomly and consistently place users into test groups\n",
    "1. Allow new users to get into existing groups as they arrive\n",
    "1. Make sure those assignments don't change over time\n",
    "\n",
    "This turns out to be very similar to what we have laid out above and we can show it with a quick, familiar query. Our goal is to split our 10,000 users into two evenly sized groups, `A` and `B`:\n",
    "\n",
    "```sql\n",
    "WITH src AS (\n",
    "  SELECT\n",
    "    user_id,\n",
    "    `project.dataset.CONSISTENT_RAND`(CONCAT(user_id, 'ab_test_1')) AS u\n",
    "  FROM UNNEST(GENERATE_ARRAY(1, 10000)) user_id\n",
    ")\n",
    "\n",
    "SELECT \n",
    "  CASE\n",
    "    WHEN u >= 0.5 THEN 'A'\n",
    "    ELSE 'B'\n",
    "  END AS test_group, \n",
    "  ARRAY_AGG(user_id) AS user_ids,\n",
    "  COUNT(user_id) AS group_size\n",
    "FROM src\n",
    "GROUP BY test_group\n",
    "ORDER BY test_group\n",
    "```\n",
    "\n",
    "If we use the combination of `user_id` and the name of the test as the salt, `ab_test_1`, then we can make sure that a user will always be in the same group for that test. And since our pipeline runs many times during the day, any new users will automatically be placed into a group during each run. Last, if we wanted to change from equal group sizes, then we can simply change the `0.5` split to any number within `[0,1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we've created a tidy UDF in BigQuery to help create random and consistent values that we can use for making A/B test groups and when working within idempotent data pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}